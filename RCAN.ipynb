{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "参考 : https://github.com/yulunzhang/RCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name_list = sorted(glob.glob('../T91/*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetTrainData(object):\n",
    "    def __init__(self, data_path_list):\n",
    "        self.data_path_list = data_path_list\n",
    "    def get_data(self):\n",
    "        X, Y = [], []\n",
    "        for i in range(len(self.data_path_list)):\n",
    "            img = Image.open(self.data_path_list[i]).convert('L')\n",
    "            (hight, width) = img.size\n",
    "            if((width<128)|(hight<128)):\n",
    "                continue\n",
    "            img_array = np.array(img).astype(np.uint8)\n",
    "            for w in range(0, width-128+1, 64):\n",
    "                for h in range(0, hight-128+1, 64):\n",
    "                    batch_img_array = img_array[w:w+128, h:h+128]\n",
    "                    Y.append(batch_img_array.reshape(128,128,1))\n",
    "                    batch_img = Image.fromarray(batch_img_array)\n",
    "                    batch_img = batch_img.resize((64, 64), Image.BICUBIC)\n",
    "                    batch_img = np.array(batch_img).astype(np.uint8)\n",
    "                    X.append(batch_img.reshape(64,64,1))\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "        return X/255.0, Y/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = GetTrainData(data_name_list).get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR image size = (501, 64, 64, 1)\n",
      "HR image size = (501, 128, 128, 1)\n",
      "LR image size = (501, 1, 64, 64)\n",
      "HR image size = (501, 1, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "print(f'LR image size = {X.shape}')\n",
    "print(f'HR image size = {Y.shape}')\n",
    "X = X.transpose(0, 3, 1, 2)\n",
    "Y = Y.transpose(0, 3, 1, 2)\n",
    "print(f'LR image size = {X.shape}')\n",
    "print(f'HR image size = {Y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Upsampler(nn.Sequential):\n",
    "    def __init__(self, conv, scale, n_feat, bn = False, act = False, bias = True):\n",
    "        m = []\n",
    "        if(scale&(scale-1))==0:\n",
    "            for _ in range(int(math.log(scale, 2))):\n",
    "                m.append(conv(n_feat, 4*n_feat, 3, bias))\n",
    "                m.append(nn.PixelShuffle(2))\n",
    "                if bn: \n",
    "                    m.append(nn.BatchNorm2d(n_feat))\n",
    "                if act:\n",
    "                    m.append(act())\n",
    "        elif scale==3:\n",
    "            m.append(conv(n_feat, 9*n_feat, 3, bias))\n",
    "            m.append(nn.PixelShuffle(3))\n",
    "            if bn:\n",
    "                m.append(nn.BatchNorm2d(n_feat))\n",
    "            if act:\n",
    "                m.append(act())\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        super(Upsampler, self).__init__(*m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CALayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv_du = nn.Sequential(\n",
    "            nn.Conv2d(channel, channel//reduction, 1, padding=0, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channel//reduction, channel, 1, padding=0, bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = self.conv_du(y)\n",
    "        return x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCAB(nn.Module):\n",
    "    def __init__(self, conv, n_feat, kernel_size, reduction, bias = True, bn = False, act = nn.ReLU(True), res_scale=1):\n",
    "        super(RCAB, self).__init__()\n",
    "        \n",
    "        modules_body = []\n",
    "        for i in range(2):\n",
    "            modules_body.append(conv(n_feat, n_feat, kernel_size, bias = bias))\n",
    "            if bn:\n",
    "                modules_body.append(nn.BatchNorm2d(n_feat))\n",
    "            if i==0:\n",
    "                modules_body.append(act)\n",
    "        modules_body.append(CALayer(n_feat, reduction))\n",
    "        self.body = nn.Sequential(*modules_body)\n",
    "        self.res_scale = res_scale\n",
    "    def forward(self, x):\n",
    "        res = self.body(x)\n",
    "        res += x\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualGroup(nn.Module):\n",
    "    def __init__(self, conv, n_feat, kernel_size, reduction, act, res_scale, n_resblocks):\n",
    "        super().__init__()\n",
    "        modules_body = []\n",
    "        modules_body = [\n",
    "            RCAB(conv, n_feat, kernel_size, reduction, bias = True, bn =False, act = nn.ReLU(True), res_scale = 1) for _ in range(n_resblocks) \n",
    "        ]\n",
    "        modules_body.append(conv(n_feat, n_feat, kernel_size))\n",
    "        self.body = nn.Sequential(*modules_body)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = self.body(x)\n",
    "        res += x\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_conv(in_channels, out_channels, kernel_size, bias = True):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size, padding = (kernel_size//2), bias = bias)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_colors, n_resgroups, n_resblocks, n_feats, reduction,  scale, res_scale, conv = default_conv):\n",
    "        super().__init__()\n",
    "        \n",
    "        kernel_size = 3\n",
    "        act = nn.ReLU(True)\n",
    "        \n",
    "        modules_head = [conv(n_colors, n_feats, kernel_size)]\n",
    "        modules_body = [\n",
    "            ResidualGroup(conv, n_feats, kernel_size, reduction, act = act, res_scale = res_scale, n_resblocks = n_resblocks) for _ in range(n_resgroups)\n",
    "        ]\n",
    "        \n",
    "        modules_body.append(conv(n_feats, n_feats, kernel_size))\n",
    "        \n",
    "        modules_tail = [\n",
    "            Upsampler(conv, scale, n_feats, act = False),\n",
    "            conv(n_feats, n_colors, kernel_size)\n",
    "        ]\n",
    "        \n",
    "        self.head = nn.Sequential(*modules_head)\n",
    "        self.body = nn.Sequential(*modules_body)\n",
    "        self.tail = nn.Sequential(*modules_tail)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.head(x)\n",
    "        \n",
    "        res = self.body(x)\n",
    "        res += x\n",
    "        \n",
    "        x = self.tail(res)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "class Mydatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y=None):\n",
    "        self.data = x\n",
    "        if y is not None:\n",
    "            self.label = y\n",
    "        else:\n",
    "            self.label = None\n",
    "\n",
    "        self.datanum = x.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_data = torch.from_numpy(self.data[idx])\n",
    "        if self.label is not None:\n",
    "            out_label = torch.from_numpy(self.label[idx])\n",
    "            return out_data, out_label\n",
    "        else:\n",
    "            return out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "            \n",
    "            \n",
    "class RCAN(object):\n",
    "    def __init__(self, MAX_EPOCH = 100, BATCH_SIZE = 32, lr = 0.00001, upscale=2, d=48, s=12, m=2):\n",
    "        self.MAX_EPOCH = MAX_EPOCH\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.lr = lr\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.model = Model(n_colors=1, n_resgroups=2, n_resblocks=2, n_feats=64, reduction=2,  scale=2, res_scale=1, conv = default_conv)\n",
    "        self.model.apply(weights_init)\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = self.lr, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, amsgrad=False)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size = 100, gamma = 0.01)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        dataset = Mydatasets(X, Y)\n",
    "        self.loss_list = []\n",
    "        for epoch in range(self.MAX_EPOCH):\n",
    "            loader = torch.utils.data.DataLoader(dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers = 2)\n",
    "            running_loss = 0.0\n",
    "            for batch_idx, (inputs, labels) in enumerate(loader):\n",
    "                inputs, labels = Variable(inputs.float()).to(self.device), Variable(labels.float()).to(self.device)\n",
    "                \n",
    "                #zero the parameter gradients\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                #forward\n",
    "                outputs = self.model(inputs)\n",
    "                \n",
    "                #backward+optimize\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                #loss = gradient_sensitive_loss(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "            print(f'Epoch[{epoch+1}/{self.MAX_EPOCH}]  loss : {running_loss/len(loader)}')\n",
    "            self.loss_list.append(running_loss/len(loader))\n",
    "            self.scheduler.step()\n",
    "        print(f'Finish training...')\n",
    "        \n",
    "    def transform(self, X):\n",
    "        self.model.eval()\n",
    "        dataset = Mydatasets(X)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "        transformed_X = []\n",
    "        total_time = 0.0\n",
    "        for batch_idx, inputs in enumerate(loader):\n",
    "            inputs = Variable(inputs.float()).to(self.device)\n",
    "            start = time.time()\n",
    "            outputs = self.model(inputs).squeeze(0).cpu().detach().numpy()\n",
    "            end = time.time()\n",
    "            total_time+=(end-start)\n",
    "            transformed_X.append(outputs)\n",
    "        print(f'Average transform time = {total_time/len(loader)}')\n",
    "        return np.array(transformed_X)\n",
    "    \n",
    "    def save_model(self, path=None):\n",
    "        if path is None:\n",
    "            path = './model.pth'\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "        print(\"SAVE MODEL SUCCESS!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcan = RCAN(MAX_EPOCH = 200, BATCH_SIZE = 16, lr = 0.0001,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/200]  loss : 19.704859424382448\n",
      "Epoch[2/200]  loss : 1.5385871464386582\n",
      "Epoch[3/200]  loss : 0.8685214258730412\n",
      "Epoch[4/200]  loss : 0.6110734576359391\n",
      "Epoch[5/200]  loss : 0.44842894468456507\n",
      "Epoch[6/200]  loss : 0.34336212556809187\n",
      "Epoch[7/200]  loss : 0.27877776604145765\n",
      "Epoch[8/200]  loss : 0.23810729943215847\n",
      "Epoch[9/200]  loss : 0.20813189866021276\n",
      "Epoch[10/200]  loss : 0.18108301609754562\n",
      "Epoch[11/200]  loss : 0.1616389281116426\n",
      "Epoch[12/200]  loss : 0.14675945392809808\n",
      "Epoch[13/200]  loss : 0.1315316583495587\n",
      "Epoch[14/200]  loss : 0.1209555109962821\n",
      "Epoch[15/200]  loss : 0.1108209565281868\n",
      "Epoch[16/200]  loss : 0.10345213324762881\n",
      "Epoch[17/200]  loss : 0.09636175679042935\n",
      "Epoch[18/200]  loss : 0.0892292937496677\n",
      "Epoch[19/200]  loss : 0.08593877137172967\n",
      "Epoch[20/200]  loss : 0.08151182380970567\n",
      "Epoch[21/200]  loss : 0.07571697351522744\n",
      "Epoch[22/200]  loss : 0.07201872032601386\n",
      "Epoch[23/200]  loss : 0.0686193557921797\n",
      "Epoch[24/200]  loss : 0.06499593600165099\n",
      "Epoch[25/200]  loss : 0.061756569310091436\n",
      "Epoch[26/200]  loss : 0.05912155355326831\n",
      "Epoch[27/200]  loss : 0.05703671544324607\n",
      "Epoch[28/200]  loss : 0.055895643192343414\n",
      "Epoch[29/200]  loss : 0.052106208400800824\n",
      "Epoch[30/200]  loss : 0.05008190940134227\n",
      "Epoch[31/200]  loss : 0.047954320441931486\n",
      "Epoch[32/200]  loss : 0.04908869101200253\n",
      "Epoch[33/200]  loss : 0.04594748973613605\n",
      "Epoch[34/200]  loss : 0.04464809293858707\n",
      "Epoch[35/200]  loss : 0.043287551787216216\n",
      "Epoch[36/200]  loss : 0.04078324610600248\n",
      "Epoch[37/200]  loss : 0.03957603860180825\n",
      "Epoch[38/200]  loss : 0.039039251452777535\n",
      "Epoch[39/200]  loss : 0.03729170688893646\n",
      "Epoch[40/200]  loss : 0.03836964239599183\n",
      "Epoch[41/200]  loss : 0.03686811635270715\n",
      "Epoch[42/200]  loss : 0.03497235424583778\n",
      "Epoch[43/200]  loss : 0.03584169672103599\n",
      "Epoch[44/200]  loss : 0.033064350369386375\n",
      "Epoch[45/200]  loss : 0.031830785679630935\n",
      "Epoch[46/200]  loss : 0.030866588465869427\n",
      "Epoch[47/200]  loss : 0.03018948360113427\n",
      "Epoch[48/200]  loss : 0.029204955673776567\n",
      "Epoch[49/200]  loss : 0.029645317001268268\n",
      "Epoch[50/200]  loss : 0.03444510855479166\n",
      "Epoch[51/200]  loss : 0.03192649903940037\n",
      "Epoch[52/200]  loss : 0.027714017080143094\n",
      "Epoch[53/200]  loss : 0.026602270314469934\n",
      "Epoch[54/200]  loss : 0.02614010829711333\n",
      "Epoch[55/200]  loss : 0.028760955552570522\n",
      "Epoch[56/200]  loss : 0.026999348687240854\n",
      "Epoch[57/200]  loss : 0.024554091069148853\n",
      "Epoch[58/200]  loss : 0.02543971937848255\n",
      "Epoch[59/200]  loss : 0.02405791394994594\n",
      "Epoch[60/200]  loss : 0.02299683162709698\n",
      "Epoch[61/200]  loss : 0.022148081130580977\n",
      "Epoch[62/200]  loss : 0.022785344335716218\n",
      "Epoch[63/200]  loss : 0.021793323481688276\n",
      "Epoch[64/200]  loss : 0.057661980390548706\n",
      "Epoch[65/200]  loss : 0.04381370096234605\n",
      "Epoch[66/200]  loss : 0.02240607189014554\n",
      "Epoch[67/200]  loss : 0.02016275213100016\n",
      "Epoch[68/200]  loss : 0.01966486102901399\n",
      "Epoch[69/200]  loss : 0.01924675825284794\n",
      "Epoch[70/200]  loss : 0.019732061336981133\n",
      "Epoch[71/200]  loss : 0.01906616412452422\n",
      "Epoch[72/200]  loss : 0.01829404491581954\n",
      "Epoch[73/200]  loss : 0.018620715592987835\n",
      "Epoch[74/200]  loss : 0.017976225819438696\n",
      "Epoch[75/200]  loss : 0.017607479210710153\n",
      "Epoch[76/200]  loss : 0.017272656579734758\n",
      "Epoch[77/200]  loss : 0.017742576776072383\n",
      "Epoch[78/200]  loss : 0.01690011564642191\n",
      "Epoch[79/200]  loss : 0.01704493458964862\n",
      "Epoch[80/200]  loss : 0.016648136428557336\n",
      "Epoch[81/200]  loss : 0.018551771412603557\n",
      "Epoch[82/200]  loss : 0.02041958406334743\n",
      "Epoch[83/200]  loss : 0.017039456899510697\n",
      "Epoch[84/200]  loss : 0.016401741217123345\n",
      "Epoch[85/200]  loss : 0.015370107372291386\n",
      "Epoch[86/200]  loss : 0.015004863089416176\n",
      "Epoch[87/200]  loss : 0.014542724442435429\n",
      "Epoch[88/200]  loss : 0.014845312078250572\n",
      "Epoch[89/200]  loss : 0.017467453639255837\n",
      "Epoch[90/200]  loss : 0.027549834921956062\n",
      "Epoch[91/200]  loss : 0.015626948239514604\n",
      "Epoch[92/200]  loss : 0.01338103877787944\n",
      "Epoch[93/200]  loss : 0.013923814753070474\n",
      "Epoch[94/200]  loss : 0.014906205935403705\n",
      "Epoch[95/200]  loss : 0.013790696451906115\n",
      "Epoch[96/200]  loss : 0.012896459986222908\n",
      "Epoch[97/200]  loss : 0.012434590054908767\n",
      "Epoch[98/200]  loss : 0.01242189560434781\n",
      "Epoch[99/200]  loss : 0.012737932964228094\n",
      "Epoch[100/200]  loss : 0.013227656221715733\n",
      "Epoch[101/200]  loss : 0.012357206142041832\n",
      "Epoch[102/200]  loss : 0.012048392585711554\n",
      "Epoch[103/200]  loss : 0.011663831261103041\n",
      "Epoch[104/200]  loss : 0.011716025619534776\n",
      "Epoch[105/200]  loss : 0.011806503433035687\n",
      "Epoch[106/200]  loss : 0.011687605961924419\n",
      "Epoch[107/200]  loss : 0.011716524604707956\n",
      "Epoch[108/200]  loss : 0.011872476388816722\n",
      "Epoch[109/200]  loss : 0.01171525035169907\n",
      "Epoch[110/200]  loss : 0.011687555248499848\n",
      "Epoch[111/200]  loss : 0.011649116713670082\n",
      "Epoch[112/200]  loss : 0.011742121918359771\n",
      "Epoch[113/200]  loss : 0.01185072485532146\n",
      "Epoch[114/200]  loss : 0.011693510721670464\n",
      "Epoch[115/200]  loss : 0.011755169383832254\n",
      "Epoch[116/200]  loss : 0.011613176829996519\n",
      "Epoch[117/200]  loss : 0.011771268968004733\n",
      "Epoch[118/200]  loss : 0.011768547381507233\n",
      "Epoch[119/200]  loss : 0.01175277674337849\n",
      "Epoch[120/200]  loss : 0.011806333059212193\n",
      "Epoch[121/200]  loss : 0.011712313716998324\n",
      "Epoch[122/200]  loss : 0.011673641944071278\n",
      "Epoch[123/200]  loss : 0.011707342404406518\n",
      "Epoch[124/200]  loss : 0.011672064574668184\n",
      "Epoch[125/200]  loss : 0.011706575824064203\n",
      "Epoch[126/200]  loss : 0.011921547586098313\n",
      "Epoch[127/200]  loss : 0.0117412172985496\n",
      "Epoch[128/200]  loss : 0.011662452670861967\n",
      "Epoch[129/200]  loss : 0.011637463787337765\n",
      "Epoch[130/200]  loss : 0.01176451487117447\n",
      "Epoch[131/200]  loss : 0.011606463012867607\n",
      "Epoch[132/200]  loss : 0.011707049241522327\n",
      "Epoch[133/200]  loss : 0.011570378090254962\n",
      "Epoch[134/200]  loss : 0.011762280249968171\n",
      "Epoch[135/200]  loss : 0.011558817568584345\n",
      "Epoch[136/200]  loss : 0.011842387059004977\n",
      "Epoch[137/200]  loss : 0.011831496201921254\n",
      "Epoch[138/200]  loss : 0.011818193655926734\n",
      "Epoch[139/200]  loss : 0.011651246415567584\n",
      "Epoch[140/200]  loss : 0.011747626311262138\n",
      "Epoch[141/200]  loss : 0.01174589064612519\n",
      "Epoch[142/200]  loss : 0.011693421445670538\n",
      "Epoch[143/200]  loss : 0.011649360385490581\n",
      "Epoch[144/200]  loss : 0.011650466360151768\n",
      "Epoch[145/200]  loss : 0.01186841880553402\n",
      "Epoch[146/200]  loss : 0.011594947514822707\n",
      "Epoch[147/200]  loss : 0.011545889516128227\n",
      "Epoch[148/200]  loss : 0.011502253240905702\n",
      "Epoch[149/200]  loss : 0.01177256225491874\n",
      "Epoch[150/200]  loss : 0.011730548198102042\n",
      "Epoch[151/200]  loss : 0.011729967285646126\n",
      "Epoch[152/200]  loss : 0.011849770089611411\n",
      "Epoch[153/200]  loss : 0.011661852593533695\n",
      "Epoch[154/200]  loss : 0.011597992677707225\n",
      "Epoch[155/200]  loss : 0.011560703118448146\n",
      "Epoch[156/200]  loss : 0.011837084050057456\n",
      "Epoch[157/200]  loss : 0.011613058974035084\n",
      "Epoch[158/200]  loss : 0.011678025737637654\n",
      "Epoch[159/200]  loss : 0.01166174879472237\n",
      "Epoch[160/200]  loss : 0.01202495995676145\n",
      "Epoch[161/200]  loss : 0.011434208005084656\n",
      "Epoch[162/200]  loss : 0.011598633107496426\n",
      "Epoch[163/200]  loss : 0.011682100637699477\n",
      "Epoch[164/200]  loss : 0.011694737055222504\n",
      "Epoch[165/200]  loss : 0.011638756186584942\n",
      "Epoch[166/200]  loss : 0.011729836813174188\n",
      "Epoch[167/200]  loss : 0.011542219799594022\n",
      "Epoch[168/200]  loss : 0.011654600064503029\n",
      "Epoch[169/200]  loss : 0.011451442711404525\n",
      "Epoch[170/200]  loss : 0.011667604325339198\n",
      "Epoch[171/200]  loss : 0.011762190057197586\n",
      "Epoch[172/200]  loss : 0.011686346799251623\n",
      "Epoch[173/200]  loss : 0.011414831504225731\n",
      "Epoch[174/200]  loss : 0.012044208269799128\n",
      "Epoch[175/200]  loss : 0.011534717938047834\n",
      "Epoch[176/200]  loss : 0.011537565471371636\n",
      "Epoch[177/200]  loss : 0.011478572429041378\n",
      "Epoch[178/200]  loss : 0.011501011904329062\n",
      "Epoch[179/200]  loss : 0.011781788693042472\n",
      "Epoch[180/200]  loss : 0.011516671627759933\n",
      "Epoch[181/200]  loss : 0.011589981673751026\n",
      "Epoch[182/200]  loss : 0.011466513271443546\n",
      "Epoch[183/200]  loss : 0.011465196468634531\n",
      "Epoch[184/200]  loss : 0.011469547578599304\n",
      "Epoch[185/200]  loss : 0.011459740315331146\n",
      "Epoch[186/200]  loss : 0.01143563057121355\n",
      "Epoch[187/200]  loss : 0.01153542089741677\n"
     ]
    }
   ],
   "source": [
    "rcan.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "transformed_X = rcan.transform(X)\n",
    "SR_image = transformed_X.transpose(0, 2, 3, 1)\n",
    "HR_image = Y.transpose(0, 2, 3, 1)\n",
    "for i in range(10):\n",
    "    plt.figure()\n",
    "    plt.gray()\n",
    "    plt.imshow(HR_image[i])\n",
    "    plt.figure()\n",
    "    plt.gray()\n",
    "    plt.imshow(SR_image[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
